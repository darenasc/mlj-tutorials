{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://julialang.org/assets/infra/logo.svg\" alt=\"Julia\" width=\"200\" style=\"max-width:100%;\">\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/darenasc/mlj-tutorials/master)\n",
    "\n",
    "# Julia programming language\n",
    "\n",
    "The sintax in Julia is very intuitive and similar to languages such as Matlab and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(42.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type inference \n",
    "\n",
    "Type annotation is not necessary in the code but it makes the code readable. *Type inference* is the process of identifying the types of the arguments to dispatch the right method. The *multiple dispatch* Julia feature make programs more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function function_x(x::String)\n",
    "    println(\"this is a string: $x\")\n",
    "end\n",
    "\n",
    "function function_x(x::Int)\n",
    "    println(\"$(x^2) is the square of $x\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_x(\"a string\")\n",
    "function_x(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/master/material/MLJLogo2.svg?sanitize=true\" alt=\"MLJ\" width=\"200\" style=\"max-width:100%;\">\n",
    "\n",
    "# MLJ\n",
    "\n",
    "MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing machine learning models written in Julia and other languages. MLJ is released under the MIT licensed and sponsored by the [Alan Turing Institute](https://www.turing.ac.uk/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLJ Universe\n",
    "\n",
    "The functionality of MLJ is distributed over a number of repositories\n",
    "illustrated in the dependency chart below.\n",
    "\n",
    "[MLJ](https://github.com/alan-turing-institute/MLJ) * [MLJBase](https://github.com/alan-turing-institute/MLJBase.jl) * [MLJModelInterface](https://github.com/alan-turing-institute/MLJModelInterface.jl) * [MLJModels](https://github.com/alan-turing-institute/MLJModels.jl) * [MLJTuning](https://github.com/alan-turing-institute/MLJTuning.jl) * [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl) * [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl) * [MLJTutorials](https://github.com/alan-turing-institute/MLJTutorials) * [MLJScientificTypes](https://github.com/alan-turing-institute/MLJScientificTypes.jl) * [ScientificTypes](https://github.com/alan-turing-institute/ScientificTypes.jl)\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://github.com/alan-turing-institute/MLJ.jl/raw/master/material/MLJ_stack.svg?sanitize=true\" alt=\"Dependency Chart\">\n",
    "</div>\n",
    "\n",
    "*Dependency chart for MLJ repositories. Repositories with dashed\n",
    "connections do not currently exist but are planned/proposed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ provides access to to a wide variety of machine learning models. For the most up-to-date list of available models `models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit, predict, transform\n",
    "\n",
    "The following example is using the `fit()`, `predict()`, and `transform()` functions of MLJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Statistics\n",
    "using PrettyPrinting\n",
    "using StableRNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = @load_iris;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's also load the DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLJ Machine\n",
    "\n",
    "In MLJ, a *model* is an object that only serves as a container for the hyperparameters of the model. A *machine* is an object wrapping both a model and data and can contain information on the *trained* model; it does *not* fit the model by itself. However, it does check that the model is compatible with the scientific type of the data and will warn you otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = machine(tree_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine is used both for supervised and unsupervised model. In this tutorial we give an example for the supervised model first and then go on with the unsupervised case.\n",
    "\n",
    "## Training and testing a supervised model\n",
    "\n",
    "Now that you've declared the model you'd like to consider and the data, we are left with the standard training and testing step for a supervised learning algorithm.\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "To split the data into a training and testing set, you can use the function `partition` to obtain indices for data points that should be considered either as training or testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = StableRNG(566)\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=rng)\n",
    "test[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and testing the machine\n",
    "\n",
    "To fit the machine, you can use the function `fit!` specifying the rows to be used for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(tree, rows=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this **modifies** the machine which now contains the trained parameters of the decision tree. You can inspect the result of the fitting with the `fitted_params` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(tree) |> pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `fitresult` will vary from model to model though classifiers will usually give out a tuple with the first element corresponding to the fitting and the second one keeping track of how classes are named (so that predictions can be appropriately named).\n",
    "\n",
    "You can now use the machine to make predictions with the `predict` function specifying rows to be used for the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = predict(tree, rows=test)\n",
    "@show ŷ[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output is probabilistic, effectively a vector with a score for each class. You could get the mode by using the `mode` function on `ŷ` or using `predict_mode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ȳ = predict_mode(tree, rows=test)\n",
    "@show ȳ[1]\n",
    "@show mode(ŷ[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the discrepancy between ŷ and y you could use the average cross entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mce = cross_entropy(ŷ, y[test]) |> mean\n",
    "round(mce, digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more advanced example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using StableRNGs\n",
    "import DataFrames\n",
    "@load RidgeRegressor pkg=MultivariateStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will show how to generate a model from a network; there are two approaches:\n",
    "\n",
    "* using the `@from_network` macro\n",
    "* writing the model in full\n",
    "\n",
    "the first approach should usually be the one considered as it's simpler.\n",
    "\n",
    "Generating a model from a network allows subsequent composition of that network with other tasks and tuning of that network.\n",
    "\n",
    "### Using the @from_network macro\n",
    "\n",
    "Let's define a simple network\n",
    "\n",
    "*Input layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = StableRNG(6616) # for reproducibility\n",
    "x1 = rand(rng, 300)\n",
    "x2 = rand(rng, 300)\n",
    "x3 = rand(rng, 300)\n",
    "y = exp.(x1 - x2 -2x3 + 0.1*rand(rng, 300))\n",
    "X = DataFrames.DataFrame(x1=x1, x2=x2, x3=x3)\n",
    "test, train = partition(eachindex(y), 0.8);\n",
    "\n",
    "Xs = source(X)\n",
    "ys = source(y, kind=:target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model = Standardizer()\n",
    "stand = machine(std_model, Xs)\n",
    "W = MLJ.transform(stand, Xs)\n",
    "\n",
    "box_model = UnivariateBoxCoxTransformer()\n",
    "box = machine(box_model, ys)\n",
    "z = MLJ.transform(box, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = RidgeRegressor(lambda=0.1)\n",
    "ridge = machine(ridge_model, W, z)\n",
    "ẑ = predict(ridge, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = inverse_transform(box, ẑ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fitting has been done thus far, we have just defined a sequence of operations.\n",
    "\n",
    "To form a model out of that network is easy using the `@from_network` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@from_network CompositeModel(std=std_model, box=box_model,\n",
    "                             ridge=ridge_model) <= ŷ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The macro defines a constructor CompositeModel and attributes a name to the different models; the ordering / connection between the nodes is inferred from `ŷ` via the `<= ŷ`.\n",
    "\n",
    "**Note**: had the model been probabilistic (e.g. `RidgeClassifier`) you would have needed to add `is_probabilistic=true` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = machine(CompositeModel(), X, y)\n",
    "res = evaluate!(cm, resampling=Holdout(fraction_train=0.8, rng=51),\n",
    "                measure=rms)\n",
    "round(res.measurement[1], sigdigits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can check more [Data Science tutorials in Julia](https://alan-turing-institute.github.io/DataScienceTutorials.jl/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
